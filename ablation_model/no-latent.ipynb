{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T07:31:55.711864Z",
     "iopub.status.busy": "2022-03-17T07:31:55.711Z",
     "iopub.status.idle": "2022-03-17T07:31:55.723869Z",
     "shell.execute_reply": "2022-03-17T07:31:55.722961Z",
     "shell.execute_reply.started": "2022-03-17T07:31:55.711829Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-12, affine=True):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "            self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = [-1] + [1] * (x.dim() - 1)\n",
    "        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
    "        std = x.view(x.size(0), -1).std(1).view(*shape)\n",
    "\n",
    "        y = (x - mean) / (std + self.eps)\n",
    "        if self.affine:\n",
    "            shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "            y = self.gamma.view(*shape) * y + self.beta.view(*shape)\n",
    "            \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T07:31:58.977376Z",
     "iopub.status.busy": "2022-03-17T07:31:58.976915Z",
     "iopub.status.idle": "2022-03-17T07:31:59.000274Z",
     "shell.execute_reply": "2022-03-17T07:31:58.998899Z",
     "shell.execute_reply.started": "2022-03-17T07:31:58.977337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "# VGG architecter, used for the perceptual loss using a pretrained VGG network\n",
    "class VGG19(nn.Module):\n",
    "    def __init__(self, requires_grad=False):\n",
    "        super().__init__()\n",
    "        vgg_pretrained_features = models.vgg19(pretrained=True).features\n",
    "        self.slice1 = nn.Sequential()\n",
    "        self.slice2 = nn.Sequential()\n",
    "        self.slice3 = nn.Sequential()\n",
    "        self.slice4 = nn.Sequential()\n",
    "        self.slice5 = nn.Sequential()\n",
    "        for x in range(2):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(2, 7):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(7, 12):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(12, 21):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(21, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h_input = X\n",
    "        h_relu1 = self.slice1(h_input)\n",
    "        h_relu2 = self.slice2(h_relu1)\n",
    "        h_relu3 = self.slice3(h_relu2)\n",
    "        h_relu4 = self.slice4(h_relu3)\n",
    "        h_relu5 = self.slice5(h_relu4)\n",
    "        out = [h_input, h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T07:32:02.066188Z",
     "iopub.status.busy": "2022-03-17T07:32:02.065409Z",
     "iopub.status.idle": "2022-03-17T07:32:02.09879Z",
     "shell.execute_reply": "2022-03-17T07:32:02.097285Z",
     "shell.execute_reply.started": "2022-03-17T07:32:02.066136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "Norm = LayerNorm\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(nn.InstanceNorm2d(in_dim, affine=True),\n",
    "                                  nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                  nn.Conv2d(in_dim, in_dim, kernel_size=3, stride=1, padding=1),\n",
    "                                  nn.InstanceNorm2d(in_dim, affine=True),\n",
    "                                  nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                  nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
    "                                  nn.AvgPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        \n",
    "        self.short_cut = nn.Sequential(nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n",
    "                                       nn.Conv2d(in_dim, out_dim, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x) + self.short_cut(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv2DLReLU(nn.Module):\n",
    "    def __init__(self, inc, outc, kernel_size=3, stride=1, padding=0, negative_slope=0.2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(inc, outc, kernel_size, stride, padding)\n",
    "        self.ln = Norm(outc)\n",
    "        self.llr = nn.LeakyReLU(negative_slope=negative_slope, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.llr(self.ln(self.conv(x)))\n",
    "\n",
    "\n",
    "class Conv2DInstLReLU(nn.Module):\n",
    "    def __init__(self, inc, outc, kernel_size=3, stride=1, padding=0, negative_slope=0.2, is_inst=True):\n",
    "        super().__init__()\n",
    "        self.is_inst = is_inst\n",
    "        self.conv = nn.Conv2d(inc, outc, kernel_size, stride, padding)\n",
    "        self.inst = nn.InstanceNorm2d(outc, affine=True)\n",
    "        self.llr = nn.LeakyReLU(negative_slope=negative_slope, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.is_inst:\n",
    "            return self.llr(self.inst(self.conv(x)))\n",
    "        else:\n",
    "            return self.llr(self.conv(x))\n",
    "\n",
    "\n",
    "class Conv2DTransposeLReLU(nn.Module):\n",
    "    def __init__(self, inc, outc, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.deconv = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.deconv = nn.ConvTranspose2d(inc, outc, kernel_size=2, stride=2, padding=0)\n",
    "        self.ln = Norm(outc)\n",
    "        self.llr = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.llr(self.ln(self.deconv(x)))\n",
    "\n",
    "\n",
    "class SwishMod(nn.Module):\n",
    "    def __init__(self, inc, outc):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(inc, outc, 3, 1, 1)\n",
    "        self.ln = Norm(outc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _x = torch.sigmoid(self.ln(self.conv(x)))\n",
    "        return x.mul(_x)\n",
    "\n",
    "\n",
    "class SwishGatedBlock(nn.Module):\n",
    "    def __init__(self, inc, outc, cat=False, conv1x1=True, dropout=False):\n",
    "        super().__init__()\n",
    "        self.conv1x1 = conv1x1\n",
    "\n",
    "        if conv1x1:\n",
    "            self.conv0 = Conv2DLReLU(inc, outc, padding=1)\n",
    "            inc = outc\n",
    "            self.conv1 = Conv2DLReLU(inc, outc, padding=1)\n",
    "        else:\n",
    "            self.conv1 = Conv2DLReLU(inc, outc, padding=1)\n",
    "        self.conv2 = Conv2DLReLU(outc, outc, padding=1)\n",
    "\n",
    "        self.pooling = nn.MaxPool2d(2)\n",
    "        if cat:\n",
    "            self.deconv1 = Conv2DTransposeLReLU(outc, outc)\n",
    "            self.deconv2 = Conv2DTransposeLReLU(inc, outc)\n",
    "            self.swish_mod = SwishMod(outc, outc)\n",
    "        else:\n",
    "            self.swish_mod = SwishMod(inc, inc)\n",
    "\n",
    "    def forward(self, inputs, cat=None):\n",
    "        if self.conv1x1:\n",
    "            inputs = self.conv0(inputs)\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        if cat is None:\n",
    "            # downsampling\n",
    "            sgb_op = self.pooling(x)\n",
    "            swish = self.pooling(inputs)\n",
    "            swish = self.swish_mod(swish)\n",
    "            concat = [sgb_op, swish]\n",
    "        else:\n",
    "            sgb_op = self.deconv1(x)\n",
    "            swish = self.deconv2(inputs)\n",
    "            swish = self.swish_mod(swish)\n",
    "            concat = [sgb_op, swish, cat]\n",
    "\n",
    "        return torch.cat(concat, dim=1), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T07:32:05.649709Z",
     "iopub.status.busy": "2022-03-17T07:32:05.649417Z",
     "iopub.status.idle": "2022-03-17T07:32:05.676936Z",
     "shell.execute_reply": "2022-03-17T07:32:05.67545Z",
     "shell.execute_reply.started": "2022-03-17T07:32:05.649678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "# Source from \"https://github.com/ultralytics/yolov5/blob/master/utils/torch_utils.py\"\n",
    "def init_torch_seeds(seed: int = 0):\n",
    "    r\"\"\" Sets the seed for generating random numbers. Returns a\n",
    "    Args:\n",
    "        seed (int): The desired seed.\n",
    "    \"\"\"\n",
    "    # Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    if seed == 0:  # slower, more reproducible\n",
    "        cudnn.deterministic = True\n",
    "        cudnn.benchmark = False\n",
    "    else:  # faster, less reproducible\n",
    "        cudnn.deterministic = False\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    print(\"Initialize random seed.\")\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "'''\n",
    "    < var >\n",
    "    Convert tensor to Variable\n",
    "'''\n",
    "def var(tensor, requires_grad=True):\n",
    "    if torch.cuda.is_available():\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        dtype = torch.FloatTensor\n",
    "    var = Variable(tensor.type(dtype), requires_grad=requires_grad)\n",
    "    return var\n",
    "\n",
    "'''\n",
    "    < make_img >\n",
    "    Generate images\n",
    "\n",
    "    * Parameters\n",
    "    dloader : Data loader for test data set\n",
    "    G : Generator\n",
    "    z : random_z(size = (N, img_num, z_dim))\n",
    "        N : test img number / img_num : Number of images that you want to generate with one test img / z_dim : 8\n",
    "    img_num : Number of images that you want to generate with one test img\n",
    "'''\n",
    "def make_img(dloader, G, z, img_num=5, img_size=128):\n",
    "    if torch.cuda.is_available():\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        dtype = torch.FloatTensor\n",
    "        \n",
    "    dloader = iter(dloader)\n",
    "    img, _ = dloader.next()\n",
    "\n",
    "    N = img.size(0)    \n",
    "    img = var(img.type(dtype))\n",
    "    result_img = torch.FloatTensor(N * (img_num + 1), 3, img_size, img_size).type(dtype)\n",
    "\n",
    "    for i in range(N):\n",
    "        # original image to the leftmost\n",
    "        result_img[i * (img_num + 1)] = img[i].data\n",
    "\n",
    "        # Insert generated images to the next of the original image\n",
    "        for j in range(img_num):\n",
    "            img_ = img[i].unsqueeze(dim=0)\n",
    "            z_ = z[i, j, :].unsqueeze(dim=0)\n",
    "            \n",
    "            out_img = G(img_, z_)\n",
    "            result_img[i * (img_num + 1) + j + 1] = out_img.data\n",
    "\n",
    "    # [-1, 1] -> [0, 1]\n",
    "    result_img = result_img / 2 + 0.5\n",
    "    return result_img\n",
    "\n",
    "\n",
    "'''\n",
    "    < make_interpolation >\n",
    "    Make linear interpolated latent code.\n",
    "    \n",
    "    * Parameters\n",
    "    n : Input images number\n",
    "    img_num : Generated images number per one input image\n",
    "    z_dim : Dimension of latent code. Basically 8.\n",
    "'''\n",
    "def make_interpolation(n=200, img_num=9, z_dim=8):\n",
    "    if torch.cuda.is_available() is True:\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    # Make interpolated z\n",
    "    step = 1 / (img_num - 1)\n",
    "    alpha = torch.from_numpy(np.arange(0, 1, step))\n",
    "    interpolated_z = torch.FloatTensor(n, img_num, z_dim).type(dtype)\n",
    "\n",
    "    for i in range(n):\n",
    "        first_z = torch.randn(1, z_dim)\n",
    "        last_z = torch.randn(1, z_dim)\n",
    "        \n",
    "        for j in range(img_num - 1):\n",
    "            interpolated_z[i, j] = (1 - alpha[j]) * first_z + alpha[j] * last_z\n",
    "        interpolated_z[i, img_num-1] = last_z\n",
    "    \n",
    "    return interpolated_z\n",
    "\n",
    "\n",
    "'''\n",
    "    < make_z >\n",
    "    Make latent code\n",
    "    \n",
    "    * Parameters\n",
    "    n : Input images number\n",
    "    img_num : Generated images number per one input image\n",
    "    z_dim : Dimension of latent code. Basically 8.\n",
    "    sample_type : random or interpolation\n",
    "'''\n",
    "def make_z(n, img_num, z_dim=8, sample_type='random'):\n",
    "    if sample_type == 'random':\n",
    "        z = var(torch.randn(n, img_num, 8))\n",
    "    elif sample_type == 'interpolation':\n",
    "        z = var(make_interpolation(n=n, img_num=img_num, z_dim=z_dim))\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T07:32:09.699958Z",
     "iopub.status.busy": "2022-03-17T07:32:09.69934Z",
     "iopub.status.idle": "2022-03-17T07:32:09.743679Z",
     "shell.execute_reply": "2022-03-17T07:32:09.742518Z",
     "shell.execute_reply.started": "2022-03-17T07:32:09.699857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Latent_Discriminator(nn.Module):\n",
    "    def __init__(self, z_dim=8, n_filters=64, negative_slope=0.2):\n",
    "        super().__init__()\n",
    "        # Discriminator with latent space z # (N, 8)\n",
    "        self.dz = nn.Sequential(\n",
    "            nn.Linear(z_dim, n_filters, bias=True),\n",
    "            nn.LayerNorm(n_filters),\n",
    "            nn.LeakyReLU(negative_slope, inplace=True),\n",
    "            nn.Linear(n_filters, n_filters, bias=True),\n",
    "            nn.LayerNorm(n_filters),\n",
    "            nn.LeakyReLU(negative_slope, inplace=True),\n",
    "            nn.Linear(n_filters, n_filters, bias=True),\n",
    "            nn.LayerNorm(n_filters),\n",
    "            nn.LeakyReLU(negative_slope, inplace=True),\n",
    "            nn.Linear(n_filters, n_filters, bias=True),\n",
    "            nn.LayerNorm(n_filters),\n",
    "            nn.LeakyReLU(negative_slope, inplace=True),\n",
    "            nn.Linear(n_filters, 1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dz(x)\n",
    "        return out \n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_nc=3, dim=64, z_dim=8):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        # n, 3, 128, 128 -> n, 256, 1, 1\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Conv2d(in_nc, dim // 2, kernel_size=7, stride=2, padding=3),\n",
    "            ResBlock(dim // 2, dim),\n",
    "            ResBlock(dim, dim * 2),\n",
    "            ResBlock(dim * 2, dim * 4),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        )\n",
    "        # n, 256, 1, 1 -> n, 256 -> n, 8\n",
    "        self.fc_mu = nn.Linear(dim * 4, z_dim)\n",
    "        # n, 512, 1, 1 -> n, 512 -> n, 8\n",
    "        self.fc_logvar = nn.Linear(dim * 4, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # n, 3, 128, 128 -> n, 256, 1, 1\n",
    "        encode = self.encode(x)\n",
    "        # n, 256, 1, 1 -> n, 256\n",
    "        encode = torch.flatten(encode, start_dim=1)\n",
    "        # get mu n, 256 -> n, 8\n",
    "        mu = self.fc_mu(encode)\n",
    "        # get logvar n, 256 -> n, 8\n",
    "        log_var = self.fc_logvar(encode)\n",
    "        # use reparameter trick\n",
    "        encoded_z = self.reparemeterize(mu, log_var, self.z_dim)\n",
    "        return (mu, log_var, encoded_z)\n",
    "        \n",
    "    # define reparameter tricks for latent space z\n",
    "    def reparemeterize(self, mu, log_variance, z_dim):\n",
    "        std = torch.exp(log_variance / 2)\n",
    "        random_z = var(torch.randn(1, z_dim))\n",
    "        encoded_z = (random_z * std) + mu\n",
    "        return encoded_z\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=8):\n",
    "        super().__init__()\n",
    "\n",
    "        down_in_channels = [3 + z_dim, 75, 256, 384, 512, 640]\n",
    "        down_out_channels = [64, 128, 192, 256, 320, 384]\n",
    "        up_in_channels = [768, 1152, 960, 768, 576, 384]\n",
    "        up_out_channels = [384, 320, 256, 192, 128, 64]\n",
    "\n",
    "        self.down0 = SwishGatedBlock(down_in_channels[0], down_out_channels[0], conv1x1=False)\n",
    "        self.down1 = SwishGatedBlock(down_in_channels[1], down_out_channels[1])\n",
    "        self.down2 = SwishGatedBlock(down_in_channels[2], down_out_channels[2])\n",
    "        self.down3 = SwishGatedBlock(down_in_channels[3], down_out_channels[3])\n",
    "        self.down4 = SwishGatedBlock(down_in_channels[4], down_out_channels[4])\n",
    "        self.down5 = SwishGatedBlock(down_in_channels[5], down_out_channels[5])\n",
    "\n",
    "        self.swishmod0 = SwishMod(down_out_channels[0], down_out_channels[0])\n",
    "        self.swishmod1 = SwishMod(down_out_channels[1], down_out_channels[1])\n",
    "        self.swishmod2 = SwishMod(down_out_channels[2], down_out_channels[2])\n",
    "        self.swishmod3 = SwishMod(down_out_channels[3], down_out_channels[3])\n",
    "        self.swishmod4 = SwishMod(down_out_channels[4], down_out_channels[4])\n",
    "        self.swishmod5 = SwishMod(down_out_channels[5], down_out_channels[5])\n",
    "\n",
    "        self.up0 = SwishGatedBlock(up_in_channels[0], up_out_channels[0], cat=True)\n",
    "        self.up1 = SwishGatedBlock(up_in_channels[1], up_out_channels[1], cat=True)\n",
    "        self.up2 = SwishGatedBlock(up_in_channels[2], up_out_channels[2], cat=True)\n",
    "        self.up3 = SwishGatedBlock(up_in_channels[3], up_out_channels[3], cat=True)\n",
    "        self.up4 = SwishGatedBlock(up_in_channels[4], up_out_channels[4], cat=True)\n",
    "        self.up5 = SwishGatedBlock(up_in_channels[5], up_out_channels[5], cat=True)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            Conv2DLReLU(down_out_channels[0] * 3, down_out_channels[0], kernel_size=1),\n",
    "            Conv2DLReLU(down_out_channels[0], down_out_channels[0], kernel_size=3, padding=1),\n",
    "            Conv2DLReLU(down_out_channels[0], down_out_channels[0], kernel_size=3, padding=1),\n",
    "            nn.Conv2d(down_out_channels[0], 3, kernel_size=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        # print('Encoder')\n",
    "        # z : (N, z_dim) -> (N, z_dim, 1, 1) -> (N, z_dim, H, W)\n",
    "        # x_with_z : (N, 3 + z_dim, H, W)\n",
    "        z = z.unsqueeze(dim=2).unsqueeze(dim=3)\n",
    "        z = z.expand(z.size(0), z.size(1), x.size(2), x.size(3))\n",
    "        x_with_z = torch.cat([x, z], dim=1)\n",
    "\n",
    "        # [B, 1, 128, 128] -> [B, 65, 64, 64] + [2, 64, 128, 128]\n",
    "        inputs, conv0 = self.down0(x_with_z)\n",
    "        # [B, 65, 64, 64] -> [B, 256, 32, 32] + [2, 128, 64, 64]\n",
    "        inputs, conv1 = self.down1(inputs) \n",
    "        # [B, 256, 32, 32] -> [B, 384, 16, 16] + [2, 192, 32, 32]\n",
    "        inputs, conv2 = self.down2(inputs)\n",
    "        # [B, 384, 16, 16] -> [B, 512, 8, 8] + [2, 256, 16, 16]\n",
    "        inputs, conv3 = self.down3(inputs)\n",
    "        # [B, 512, 8, 8] -> [B, 640, 4, 4] + [2, 320, 8, 8]\n",
    "        inputs, conv4 = self.down4(inputs)\n",
    "        # [B, 640, 4, 4] -> [B, 768, 2, 2] + [2, 384, 4, 4]\n",
    "        inputs, conv5 = self.down5(inputs)\n",
    "\n",
    "        # print('SwishMod')\n",
    "        # [2, 64, 128, 128]\n",
    "        conv0 = self.swishmod0(conv0)\n",
    "        # [2, 128, 64, 64]\n",
    "        conv1 = self.swishmod1(conv1)\n",
    "        # [2, 192, 32, 32]\n",
    "        conv2 = self.swishmod2(conv2)\n",
    "        # [2, 256, 16, 16]\n",
    "        conv3 = self.swishmod3(conv3)\n",
    "        # [2, 320, 8, 8]\n",
    "        conv4 = self.swishmod4(conv4)\n",
    "        # [2, 384, 4, 4]\n",
    "        conv5 = self.swishmod5(conv5)\n",
    "\n",
    "        # print('Decoder')\n",
    "        # [B, 768, 2, 2] -> [B, 1152, 4, 4]\n",
    "        inputs, _ = self.up0(inputs, cat=conv5)\n",
    "        # [B, 1152, 4, 4] -> [B, 960, 8, 8]\n",
    "        inputs, _ = self.up1(inputs, cat=conv4)\n",
    "        # [B, 960, 8, 8] -> [B, 768, 16, 16]\n",
    "        inputs, _ = self.up2(inputs, cat=conv3)\n",
    "        # [B, 768, 16, 16] -> [B, 576, 32, 32]\n",
    "        inputs, _ = self.up3(inputs, cat=conv2)\n",
    "        # [B, 576, 32, 32] -> [B, 384, 64, 64]\n",
    "        inputs, _ = self.up4(inputs, cat=conv1)\n",
    "        # [B, 384, 64, 64] -> [B, 192, 128, 128]\n",
    "        inputs, _ = self.up5(inputs, cat=conv0)\n",
    "        # [B, 192, 128, 128] -> [B, 3, 128, 128]\n",
    "        out = self.out(inputs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_filters=64):\n",
    "        super().__init__()   \n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            Conv2DInstLReLU(inc=6, outc=n_filters, kernel_size=4, stride=2, padding=1, is_inst=False),\n",
    "            Conv2DInstLReLU(inc=n_filters, outc=n_filters * 2, kernel_size=4, stride=2, padding=1),\n",
    "            Conv2DInstLReLU(inc=n_filters * 2, outc=n_filters * 4, kernel_size=4, stride=2, padding=1),\n",
    "            Conv2DInstLReLU(inc=n_filters * 4, outc=n_filters * 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Conv2d(n_filters * 8, 1, kernel_size=4, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.out(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T07:32:14.66603Z",
     "iopub.status.busy": "2022-03-17T07:32:14.665695Z",
     "iopub.status.idle": "2022-03-17T07:32:14.687475Z",
     "shell.execute_reply": "2022-03-17T07:32:14.686484Z",
     "shell.execute_reply.started": "2022-03-17T07:32:14.666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# MSELoss for LSGAN\n",
    "def MSELoss(score, target=1):\n",
    "    if target == 1:\n",
    "        label = var(torch.ones(score.size()).fill_(0.95), requires_grad=False)\n",
    "    elif target == 0:\n",
    "        label = var(torch.ones(score.size()).fill_(0.05), requires_grad=False)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(score, label)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# BCELoss for Latent code z\n",
    "def BCELoss(score, target=1):\n",
    "    if target == 1:\n",
    "        label = var(torch.ones(score.size()).fill_(0.95), requires_grad=False)\n",
    "    elif target == 0:\n",
    "        label = var(torch.ones(score.size()).fill_(0.05), requires_grad=False)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(score, label)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# Perceptual loss that uses a pretrained VGG network\n",
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGLoss, self).__init__()\n",
    "        self.vgg = VGG19().cuda()\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.weights = [0.88, 0.79, 0.63, 0.51, 0.39, 1.07]\n",
    "        # self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "        loss = 0\n",
    "        for i in range(len(x_vgg)):\n",
    "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n",
    "        return loss\n",
    "\n",
    "# KL Divergence loss used in VAE with an image encoder\n",
    "class KLDLoss(nn.Module):\n",
    "    def forward(self, mu, logvar):\n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "\n",
    "# wgan_loss\n",
    "def WGANLoss(pred, real_or_not=True):\n",
    "    if real_or_not:\n",
    "        return - torch.mean(pred)\n",
    "    else:\n",
    "        return torch.mean(pred)\n",
    "\n",
    "\n",
    "def Calculate_gradient_penalty(model, real_images, fake_images, device, constant=1.0, lamb=10.0):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake data\n",
    "    alpha = torch.randn((real_images.size(0), 1, 1, 1), device=device)\n",
    "    # Get random interpolation between real and fake data\n",
    "    interpolates = (alpha * real_images + ((1 - alpha) * fake_images)).requires_grad_(True)\n",
    "\n",
    "    model_interpolates = model(interpolates)\n",
    "    grad_outputs = torch.ones(model_interpolates.size(), device=device, requires_grad=False)\n",
    "\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=model_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lamb\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T07:32:20.71451Z",
     "iopub.status.busy": "2022-03-17T07:32:20.714214Z",
     "iopub.status.idle": "2022-03-17T07:32:20.738026Z",
     "shell.execute_reply": "2022-03-17T07:32:20.736679Z",
     "shell.execute_reply.started": "2022-03-17T07:32:20.714479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as Transforms\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class BlackWhite2Color(Dataset):\n",
    "    def __init__(self, root, transform, mode='train'):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        data_dir = os.path.join(root, mode)\n",
    "        self.file_list = os.listdir(data_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, self.mode, self.file_list[idx])\n",
    "        img = Image.open(img_path)\n",
    "        # W, H = img.size[0], img.size[1]\n",
    "        \n",
    "        data_l = img.convert('L') # 1 dimension\n",
    "        data = [data_l, data_l, data_l]\n",
    "        data = Image.merge(\"RGB\", data)   # 3 dimension\n",
    "        ground_truth = img\n",
    "        \n",
    "        data = self.transform(data)\n",
    "        ground_truth = self.transform(ground_truth)\n",
    "        \n",
    "        return (data, ground_truth)\n",
    "\n",
    "\n",
    "class Sketch2Color(Dataset):\n",
    "    def __init__(self, root, transform, mode='train'):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        data_dir = os.path.join(root, mode)\n",
    "        self.file_list = os.listdir(data_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, self.mode, self.file_list[idx])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        W, H = img.size[0], img.size[1]\n",
    "\n",
    "        data = img.crop((int(W / 2), 0, W, H))\n",
    "        ground_truth = img.crop((0, 0, int(W / 2), H))\n",
    "        \n",
    "        data = self.transform(data)\n",
    "        ground_truth = self.transform(ground_truth)\n",
    "        \n",
    "        return (data, ground_truth)\n",
    "\n",
    "\n",
    "def data_loader(root, batch_size=1, shuffle=True, img_size=128, mode='train', dstname='sketch'):    \n",
    "    transform = Transforms.Compose([Transforms.Resize((img_size, img_size)),\n",
    "                                    Transforms.ToTensor(),\n",
    "                                    Transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                                                         std=(0.5, 0.5, 0.5))\n",
    "                                   ])\n",
    "    if dstname == 'sketch':\n",
    "        dset = Sketch2Color(root, transform, mode=mode)\n",
    "    else:\n",
    "        dset = BlackWhite2Color(root, transform, mode=mode)\n",
    "    \n",
    "    if batch_size == 'all':\n",
    "        batch_size = len(dset)\n",
    "        \n",
    "    dloader = torch.utils.data.DataLoader(dset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=shuffle,\n",
    "                                          num_workers=0,\n",
    "                                          drop_last=True)\n",
    "    dlen = len(dset)\n",
    "    \n",
    "    return dloader, dlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T07:32:26.978003Z",
     "iopub.status.busy": "2022-03-17T07:32:26.977382Z",
     "iopub.status.idle": "2022-03-17T07:32:27.033717Z",
     "shell.execute_reply": "2022-03-17T07:32:27.032676Z",
     "shell.execute_reply.started": "2022-03-17T07:32:26.977964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# for reproductionary\n",
    "init_torch_seeds(seed=1)\n",
    "\n",
    "class Solver():\n",
    "    def __init__(self, root='dataset/anime_faces', dstname='sketch', result_dir='result', weight_dir='weight', load_weight=False,\n",
    "                 batch_size=1, test_size=10, test_img_num=5, img_size=128, num_epoch=100, save_every=1000,\n",
    "                 g_lr=0.0002, d_lr=0.0001, beta_1=0.5, beta_2=0.999, lambda_kl=0.01, lambda_img=10, lambda_z=0.5, \\\n",
    "                     z_dim=8, logdir=None):\n",
    "        \n",
    "        # Data type(Can use GPU or not?)\n",
    "        self.dtype = torch.cuda.FloatTensor\n",
    "        if torch.cuda.is_available() is False:\n",
    "            self.dtype = torch.FloatTensor\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Data loader for training\n",
    "        self.dloader, dlen = data_loader(root=root, batch_size=batch_size, shuffle=True, \n",
    "                                         img_size=img_size, mode='train', dstname=dstname)\n",
    "        print('training dataset length:', dlen)\n",
    "        # Data loader for test\n",
    "        self.t_dloader, _ = data_loader(root=root, batch_size=test_size, shuffle=False, \n",
    "                                        img_size=img_size, mode='test', dstname=dstname)\n",
    "\n",
    "        # Models\n",
    "        # Di is discriminator for image\n",
    "        # Dz is discriminator for latent code\n",
    "        # G is generator for input image and latent code z\n",
    "        # Z is encoder for information difference\n",
    "        self.Di = Discriminator().type(self.dtype)\n",
    "        self.Di.apply(weights_init)\n",
    "        self.G = Generator().type(self.dtype)\n",
    "        self.G.apply(weights_init)\n",
    "        self.E = Encoder().type(self.dtype)\n",
    "        self.E.apply(weights_init)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optim_Di = optim.Adam(self.Di.parameters(), lr=d_lr, betas=(beta_1, beta_2))\n",
    "        self.optim_G = optim.Adam(self.G.parameters(), lr=g_lr, betas=(beta_1, beta_2))\n",
    "        self.optim_E = optim.Adam(self.E.parameters(), lr=g_lr, betas=(beta_1, beta_2))\n",
    "\n",
    "        # fixed random_z for test\n",
    "        self.fixed_z = var(torch.randn(test_size, test_img_num, z_dim))\n",
    "\n",
    "        # losses\n",
    "        self.bce_loss = BCELoss\n",
    "        self.recon_x_loss = VGGLoss()\n",
    "        self.recon_z_loss = nn.L1Loss()\n",
    "        self.mse_loss = MSELoss\n",
    "        self.kl_loss = KLDLoss()\n",
    "\n",
    "        # Some hyperparameters\n",
    "        self.z_dim = z_dim\n",
    "        self.lambda_img = lambda_img\n",
    "        self.lambda_kl = lambda_kl\n",
    "        self.lambda_z = lambda_z\n",
    "\n",
    "        self.writer = SummaryWriter(logdir)\n",
    "\n",
    "        # Extra things\n",
    "        self.result_dir = result_dir\n",
    "        self.weight_dir = weight_dir\n",
    "        self.load_weight = load_weight\n",
    "        self.test_img_num = test_img_num\n",
    "        self.img_size = img_size\n",
    "        self.start_epoch = 0\n",
    "        self.num_epoch = num_epoch\n",
    "        self.save_every = save_every\n",
    "    \n",
    "    '''\n",
    "        < show_model >\n",
    "        Print model architectures\n",
    "    '''\n",
    "    def show_model(self):\n",
    "        print('================================ Discriminator for image =====================================')\n",
    "        print(self.Di)\n",
    "        print('==========================================================================================\\n\\n')\n",
    "        print('================================= Generator ==================================================')\n",
    "        print(self.G)\n",
    "        print('==========================================================================================\\n\\n')\n",
    "        print('================================= Encoder ==================================================')\n",
    "        print(self.E)\n",
    "        print('==========================================================================================\\n\\n')\n",
    "        \n",
    "    '''\n",
    "        < set_train_phase >\n",
    "        Set training phase\n",
    "    '''\n",
    "    def set_train_phase(self):\n",
    "        self.Di.train()\n",
    "        self.G.train()\n",
    "        self.E.train()\n",
    "    \n",
    "    '''\n",
    "        < load_checkpoint >\n",
    "        If you want to continue to train, load pretrained weight from checkpoint\n",
    "    '''\n",
    "    def load_checkpoint(self, checkpoint):\n",
    "        print('Load model')\n",
    "        self.Di.load_state_dict(checkpoint['discriminator_image_state_dict'])\n",
    "        self.G.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        self.E.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        self.optim_Di.load_state_dict(checkpoint['optim_di'])\n",
    "        self.optim_G.load_state_dict(checkpoint['optim_g'])\n",
    "        self.optim_E.load_state_dict(checkpoint['optim_e'])\n",
    "        self.start_epoch = checkpoint['epoch']\n",
    "        \n",
    "    '''\n",
    "        < save_checkpoint >\n",
    "        Save checkpoint\n",
    "    '''\n",
    "    def save_checkpoint(self, state, file_name):\n",
    "        print('saving check_point')\n",
    "        torch.save(state, file_name)\n",
    "    \n",
    "    '''\n",
    "        < all_zero_grad >\n",
    "        Set all optimizers' grad to zero \n",
    "    '''\n",
    "    def all_zero_grad(self):\n",
    "        self.optim_Di.zero_grad()\n",
    "        self.optim_G.zero_grad()\n",
    "        self.optim_E.zero_grad()\n",
    "\n",
    "    '''\n",
    "        < train >\n",
    "        Train the D_image, D_latnet, G and E \n",
    "    '''\n",
    "    def train(self):\n",
    "        if self.load_weight is True:\n",
    "            # checkpoint = torch.load(os.path.join(self.weight_dir, '14-G.pkl'))\n",
    "            checkpoint = torch.load('../input/weightnolatent/checkpoint_49_epoch.pkl')\n",
    "            self.load_checkpoint(checkpoint)\n",
    "        \n",
    "        self.set_train_phase()\n",
    "        self.show_model()\n",
    "\n",
    "        print('====================     Training    Start... =====================')\n",
    "        for epoch in range(self.start_epoch, self.num_epoch):\n",
    "            start_time = time.time()\n",
    "\n",
    "            for iters, (img, ground_truth) in tqdm(enumerate(self.dloader)):\n",
    "                # img : (1, 3, 128, 128) of domain A / ground_truth : (1, 3, 128, 128) of domain B\n",
    "                img, ground_truth = var(img), var(ground_truth)\n",
    "          \n",
    "                # seperate data for image and z latent space\n",
    "                data = {'img' : img[0].unsqueeze(dim=0), 'ground_truth' : ground_truth[0].unsqueeze(dim=0)}\n",
    "\n",
    "                ''' ----------------------------- 1. Train D ----------------------------- '''\n",
    "                # encoded latent vector\n",
    "                _, _, z_hat = self.E(data['ground_truth'])\n",
    "                # generate fake image \n",
    "                x_tilde = self.G(data['img'], z_hat)\n",
    "\n",
    "                # random latent vector\n",
    "                z = var(torch.randn(1, self.z_dim))\n",
    "                # generate fake image \n",
    "                x_hat = self.G(data['img'], z)\n",
    "                # encoded latent vector\n",
    "                z_tilde, _, _ = self.E(x_hat)\n",
    "\n",
    "                # get scores and loss\n",
    "                real_pair = torch.cat([data['img'], data['ground_truth']], dim=1)\n",
    "                fake_pair_tilde = torch.cat([data['img'], x_tilde], dim=1)\n",
    "                fake_pair_hat = torch.cat([data['img'], x_hat], dim=1)\n",
    "\n",
    "                real_d = self.Di(real_pair)\n",
    "                fake_d_tidle = self.Di(fake_pair_tilde.detach())\n",
    "                fake_d_hat = self.Di(fake_pair_hat.detach())\n",
    "\n",
    "\n",
    "                loss_images = (self.mse_loss(real_d, target=1) * 2 + self.mse_loss(fake_d_tidle, target=0) + \\\n",
    "                                    self.mse_loss(fake_d_hat, target=0)) / 4\n",
    "                \n",
    "                d_loss = loss_images\n",
    "\n",
    "                self.writer.add_scalars('d_losses', {'images_loss': loss_images}, epoch)\n",
    "\n",
    "                # Update D\n",
    "                self.all_zero_grad()\n",
    "                d_loss.backward()\n",
    "                self.optim_Di.step()\n",
    "                \n",
    "\n",
    "                ''' ----------------------------- 2. Train G & E ----------------------------- '''\n",
    "                # encoded latent vector\n",
    "                mu, log_variance, z_hat = self.E(data['ground_truth'])\n",
    "                # generate fake image \n",
    "                x_tilde = self.G(data['img'], z_hat)\n",
    "\n",
    "                # random latent vector\n",
    "                z = var(torch.randn(1, self.z_dim))\n",
    "                # generate fake image \n",
    "                x_hat = self.G(data['img'], z)\n",
    "                # encoded latent vector\n",
    "                z_tilde, _, _ = self.E(x_hat)\n",
    "\n",
    "                # get scores and loss\n",
    "                fake_pair_tilde = torch.cat([data['img'], x_tilde], dim=1)\n",
    "                fake_pair_hat = torch.cat([data['img'], x_hat], dim=1)\n",
    "\n",
    "                fake_d_tidle = self.Di(fake_pair_tilde)\n",
    "                fake_d_hat = self.Di(fake_pair_hat)\n",
    "\n",
    "                loss_images = (self.mse_loss(fake_d_tidle, target=1) +  self.mse_loss(fake_d_hat, target=1)) / 2\n",
    "                g_loss = loss_images\n",
    "                \n",
    "                loss_x_recon = self.recon_x_loss(x_tilde, data['ground_truth']) * self.lambda_img\n",
    "                loss_z_recon = self.recon_z_loss(z, z_tilde) * self.lambda_z\n",
    "                loss_kl = self.lambda_kl * self.kl_loss(mu, log_variance)\n",
    "\n",
    "                eg_loss = g_loss + loss_x_recon + loss_z_recon + loss_kl\n",
    "\n",
    "                self.all_zero_grad()\n",
    "                eg_loss.backward()\n",
    "                self.optim_E.step()\n",
    "                self.optim_G.step()\n",
    "\n",
    "                self.writer.add_scalars('eg_losses', {'images_loss': loss_images, 'x_recon_loss': loss_x_recon, \\\n",
    "                    'z_recon_loss': loss_z_recon, 'kl_loss': loss_kl,}, epoch)\n",
    "\n",
    "                log_file = open('log.txt', 'w')\n",
    "                log_file.write(str(epoch))\n",
    "                \n",
    "                # Print error and save intermediate result image and weight\n",
    "                if iters % self.save_every == 0:\n",
    "                    et = time.time() - start_time\n",
    "                    et = str(datetime.timedelta(seconds=et))[:-7]\n",
    "                    print('[Elapsed : %s / Epoch : %d / Iters : %d] => D_loss : %f / G_loss : %f / KL_div : %f / img_recon_loss : %f / z_recon_loss : %f'\\\n",
    "                          %(et, epoch, iters, d_loss.item(), g_loss.item(), loss_kl.item(), loss_x_recon.item(), loss_z_recon.item()))\n",
    "\n",
    "                   \n",
    "                    # Save intermediate result image\n",
    "                    if os.path.exists(self.result_dir) is False:\n",
    "                        os.makedirs(self.result_dir)\n",
    "                   \n",
    "                    self.G.eval()\n",
    "                    with torch.no_grad():\n",
    "                        result_img = make_img(self.t_dloader, self.G, self.fixed_z, \n",
    "                                                img_num=self.test_img_num, img_size=self.img_size)\n",
    "\n",
    "                    img_name = '{epoch}_{iters}.png'.format(epoch=epoch, iters=iters)\n",
    "                    img_path = os.path.join(self.result_dir, img_name)\n",
    "\n",
    "                    torchvision.utils.save_image(result_img, img_path, nrow=self.test_img_num + 1)\n",
    "\n",
    "                    # Save intermediate weight\n",
    "                    if os.path.exists(self.weight_dir) is False:\n",
    "                        os.makedirs(self.weight_dir)\n",
    "                    \n",
    "            \n",
    "            # Save weight at the end of every epoch\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                # self.save_weight(epoch=epoch)\n",
    "                checkpoint = {\n",
    "                    \"generator_state_dict\": self.G.state_dict(),\n",
    "                    \"discriminator_image_state_dict\": self.Di.state_dict(),\n",
    "                    \"encoder_state_dict\": self.E.state_dict(),\n",
    "                    \"optim_g\": self.optim_G.state_dict(),\n",
    "                    \"optim_di\": self.optim_Di.state_dict(),\n",
    "                    \"optim_g\": self.optim_G.state_dict(),\n",
    "                    \"optim_e\": self.optim_E.state_dict(),\n",
    "                    \"epoch\": epoch\n",
    "                    }\n",
    "                path_checkpoint = os.path.join(self.weight_dir, \"checkpoint_{}_epoch.pkl\".format(epoch))\n",
    "                self.save_checkpoint(checkpoint, path_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T02:52:44.181297Z",
     "iopub.status.busy": "2022-03-17T02:52:44.180934Z",
     "iopub.status.idle": "2022-03-17T05:09:10.246922Z",
     "shell.execute_reply": "2022-03-17T05:09:10.245479Z",
     "shell.execute_reply.started": "2022-03-17T02:52:44.181259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def main(args):\n",
    "    solver = Solver(root = args.root,\n",
    "                    dstname= args.dstname,\n",
    "                    result_dir = args.result_dir,\n",
    "                    weight_dir = args.weight_dir,\n",
    "                    load_weight = args.load_weight,\n",
    "                    batch_size = args.batch_size,\n",
    "                    test_size = args.test_size,\n",
    "                    test_img_num = args.test_img_num,\n",
    "                    img_size = args.img_size,\n",
    "                    num_epoch = args.num_epoch,\n",
    "                    save_every = args.save_every,\n",
    "                    g_lr = args.g_lr,\n",
    "                    d_lr = args.d_lr,\n",
    "                    beta_1 = args.beta_1,\n",
    "                    beta_2 = args.beta_2,\n",
    "                    lambda_kl = args.lambda_kl,\n",
    "                    lambda_img = args.lambda_img,\n",
    "                    lambda_z = args.lambda_z,\n",
    "                    z_dim = args.z_dim)\n",
    "                    \n",
    "    solver.train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--root', type=str, default='../input/animeface', \n",
    "                        help='Data location')\n",
    "    parser.add_argument('--result_dir', type=str, default='./', \n",
    "                        help='Result images location')\n",
    "    parser.add_argument('--dstname', type=str, default='anime', \n",
    "                        help='Choosed dataset name(sketch2color/black2color)')\n",
    "    parser.add_argument('--weight_dir', type=str, default='./', \n",
    "                        help='Weight location')\n",
    "    parser.add_argument('--batch_size', type=int, default=1, \n",
    "                        help='Training batch size')\n",
    "    parser.add_argument('--test_size', type=int, default=8, \n",
    "                        help='Test batch size')\n",
    "    parser.add_argument('--test_img_num', type=int, default=8, \n",
    "                        help='How many images do you want to generate?')\n",
    "    parser.add_argument('--img_size', type=int, default=128, \n",
    "                        help='Image size')\n",
    "    parser.add_argument('--g_lr', type=float, default=0.0001,\n",
    "                        help='Learning rate')\n",
    "    parser.add_argument('--d_lr', type=float, default=0.0002,\n",
    "                        help='Discriminator Learning rate')\n",
    "    parser.add_argument('--beta_1', type=float, default=0.5, \n",
    "                        help='Beta1 for Adam')\n",
    "    parser.add_argument('--beta_2', type=float, default=0.999, \n",
    "                        help='Beta2 for Adam')\n",
    "    parser.add_argument('--lambda_kl', type=float, default=1e-2, \n",
    "                        help='Lambda for KL Divergence')\n",
    "    parser.add_argument('--lambda_img', type=float, default=10, \n",
    "                        help='Lambda for image reconstruction')\n",
    "    parser.add_argument('--lambda_z', type=float, default=0.5, \n",
    "                        help='Lambda for z reconstruction')\n",
    "    parser.add_argument('--z_dim', type=int, default=8, \n",
    "                        help='Dimension of z')\n",
    "    parser.add_argument('--num_epoch', type=int, default=100, \n",
    "                        help='Number of epoch')\n",
    "    parser.add_argument('--save_every', type=int, default=1000, \n",
    "                        help='How often do you want to see the result?')\n",
    "    parser.add_argument('--load_weight', type=bool, default=True,\n",
    "                        help='Load weight or not')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T07:32:32.99288Z",
     "iopub.status.busy": "2022-03-17T07:32:32.992264Z",
     "iopub.status.idle": "2022-03-17T07:33:12.545246Z",
     "shell.execute_reply": "2022-03-17T07:33:12.544384Z",
     "shell.execute_reply.started": "2022-03-17T07:32:32.992814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "def make_img_split(dloader, G, z, img_num=5):\n",
    "    if torch.cuda.is_available():\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        dtype = torch.FloatTensor\n",
    "        \n",
    "    dloader = iter(dloader)\n",
    "    img, img_real = dloader.next()\n",
    "\n",
    "    N = img.size(0)    \n",
    "    img = var(img.type(dtype))\n",
    "    img_real = var(img_real.type(dtype))\n",
    "\n",
    "    for i in range(N):\n",
    "        # generate img_num images per a domain B image\n",
    "        real_img = img_real[i].data / 2 + 0.5\n",
    "        img_name_ = '{idx}.png'.format(idx=str(i + 1))\n",
    "        img_path = os.path.join(args.result_dir, \"ground_truth\")\n",
    "        if os.path.exists(img_path) is False:\n",
    "            os.makedirs(img_path)\n",
    "   \n",
    "        real_img_path = os.path.join(img_path, img_name_)\n",
    "        torchvision.utils.save_image(real_img, real_img_path)\n",
    "\n",
    "        # Insert generated images to the next of the original image\n",
    "        for j in range(img_num):\n",
    "            img_ = img[i].unsqueeze(dim=0)\n",
    "            z_ = z[i, j, :].unsqueeze(dim=0)\n",
    "            \n",
    "            out_img = G(img_, z_)\n",
    "            out_img = out_img.data / 2 + 0.5\n",
    "            img_name = '{idx}.png'.format(idx=str(i * img_num + j + 1))\n",
    "            img_path = os.path.join(args.result_dir, \"generated\")\n",
    "            if os.path.exists(img_path) is False:\n",
    "                os.makedirs(img_path)\n",
    "            gen_img_path = os.path.join(img_path, img_name)\n",
    "            torchvision.utils.save_image(out_img, gen_img_path)\n",
    "\n",
    "\n",
    "def main(args):    \n",
    "    dloader, dlen = data_loader(root=args.root, batch_size=100, shuffle=False, \n",
    "                                img_size=128, mode='test', dstname='anime')\n",
    "\n",
    "    if torch.cuda.is_available() is True:\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        dtype = torch.FloatTensor\n",
    "        \n",
    "    if args.epochs is not None:\n",
    "        weight_name = 'checkpoint_{epoch}_epoch.pkl'.format(epoch=args.epochs)\n",
    "    else:\n",
    "        weight_name = 'checkpoint_1_epoch.pkl'\n",
    "        \n",
    "    checkpoint = torch.load(os.path.join(args.weight_dir, weight_name))\n",
    "    G = Generator(z_dim=8).type(dtype)\n",
    "    G.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    G.eval()\n",
    "    \n",
    "    if os.path.exists(args.result_dir) is False:\n",
    "        os.makedirs(args.result_dir)\n",
    "\n",
    "    # Make latent code and images\n",
    "    z = make_z(n=dlen, img_num=args.img_num, z_dim=8, sample_type=args.sample_type)\n",
    "\n",
    "    make_img_split(dloader, G, z, img_num=args.img_num)  \n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--sample_type', type=str, default='random',\n",
    "                        help='Type of sampling : \\'random\\' or \\'interpolation\\'') \n",
    "    parser.add_argument('--root', type=str, default='../input/anime-test', \n",
    "                        help='Data location')\n",
    "    parser.add_argument('--result_dir', type=str, default='./',\n",
    "                        help='Ouput images location')\n",
    "    parser.add_argument('--weight_dir', type=str, default='../input/weightlatent',\n",
    "                        help='Trained weight location of generator. pkl file location')\n",
    "    parser.add_argument('--img_num', type=int, default=8,\n",
    "                        help='Generated images number per one input image')\n",
    "    parser.add_argument('--epochs', type=int, default=64,\n",
    "                        help='Epoch that you want to see the result. If it is None, the most recent epoch')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T07:35:34.932215Z",
     "iopub.status.busy": "2022-03-17T07:35:34.931866Z",
     "iopub.status.idle": "2022-03-17T07:35:36.522637Z",
     "shell.execute_reply": "2022-03-17T07:35:36.521561Z",
     "shell.execute_reply.started": "2022-03-17T07:35:34.932183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r file.zip ./generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T07:33:24.239873Z",
     "iopub.status.busy": "2022-03-17T07:33:24.239529Z",
     "iopub.status.idle": "2022-03-17T07:33:24.246995Z",
     "shell.execute_reply": "2022-03-17T07:33:24.245787Z",
     "shell.execute_reply.started": "2022-03-17T07:33:24.239844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T07:33:38.907427Z",
     "iopub.status.busy": "2022-03-17T07:33:38.906717Z",
     "iopub.status.idle": "2022-03-17T07:33:38.92976Z",
     "shell.execute_reply": "2022-03-17T07:33:38.928516Z",
     "shell.execute_reply.started": "2022-03-17T07:33:38.907393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink(r'./generated')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
